v2 <- c(5,2,7,3,3,1,6,3,2)
m2 <- matrix(v2, 3,3 )
solve(m2)
v <- c(1,2,7,3,3,1,6,3,2)
v2 <- c(5,2,7,3,3,1,6,3,2)
m <- matrix(v,3,3)
m2 <- matrix(v2,3,3)
cMat <- makeCacheMatrix(m)
cMat$getinverse()
cMat2 <- cacheSolve(cMat)
cMat$getinverse()
cMat3 <- cacheSolve(cMat)
cMat$getinverse()
makeCacheMatrix <- function(x = matrix()){
inv <- NULL
set <- function(y){
x <<- y
inv <<- NULL
}
get <- function() x
setinverse <- function(inverse) inv <<- inverse
getinverse <- function() inv
list(set=set, get=get, setinverse=setinverse, getinverse=getinverse )
}
cacheSolve <- function(x, ...){
## Return a matrix that is the inverse of 'x'
inverse <- x$getinverse()
if(!is.null(inverse)){
message("using cached data")
return(inverse)
}
data <- x$get()
inverse <- solve(data, ...)
x$setinverse(inverse)
inverse
}
cMat <- makeCacheMatrix(m)
cMat$getinverse()
cMat2 <- cacheSolve(cMat)
cMat$getinverse()
cMat3 <- cacheSolve(cMat)
cMat$getinverse()
cMat$set(m2)
cMat$getinverse()
cMat4 <- cacheSolve(cMat)
cMat$getinverse()
cMat5 <- cacheSolve(cMat)
cMat$getinverse()
install.packages("swirl")
library(swirl)
swirl()
5+7
x <- 5 +7
x
y <- x - 3
y
c(1.1,9,3.14)
z <- c(1.1,9,3.14)
?c
z
z <- c(555,z)
c(z, 555,z)
z*2+100
my_sqrt <- sqrt(z-1)
my_sqrt
my_div <- z / my_sqrt
my_div
c(1,2,3,4)+c(0,10)
c(1,2,3,4)+c(0,10,100)
z*2+1000
my_div
1:20
pi:10
15:1
?`:`
seq(1,20)
seq(0,10, by=0.5)
my_seq <- seq(5,10, length=30)
length(my_seq)
1:length(my_seq)
seq(along = my_seq)
seq_along(my_seq)
rep(0, times =40)
rep(c(0,1,2),times=10)
rep(c(0,1,2), each = 10)
?qpois
set.seed(20)
x <- rnorm(100)
e <- rnorm(100, 0, 2)
y <- 0.5 + 2*x+e
summary(y)
plot(x,y)
x <- rbinom(100,1,0.5)
y <- 0.5 + 2*x+e
set.seed(10)
x <- rbinom(100,1,0.5)
e <- rnorm(100,0,2)
y <- 0.5 + 2*x+e
summary(y)
plot(x,y)
set.seed(10)
x <- rbinom(10,10,0.5)
e <- rnorm(10,0,20)
y <- 0.5 + 2*x + e
plot(x,y)
?Poisson
set.seed(1)
x <- rnorm(100)
log.mu <- 0.5 + 0.3*x
y <- rpois(100, exp(log.mu))
summary(y)
plot(x,y
)
install.packages("rJava")
install.packages("xlsx")
library(xlsx)
if(!file.exists("data")){
dir.create("data")
}
#Baltimore data files
#https://data.baltimorecity.gov/Transportation/Baltimore-Fixed-Speed-Cameras/dz54-2aru
fileUrl <- "https://data.baltimorecity.gov/api/views/dz54-2aru/rows.csv?accessType=DOWNLOAD"
#if the file is https then must use curl - if on a mac
#curl not required on windows
#download.file(fileUrl, destfile="./data/cameras.csv", method="curl")
download.file(fileUrl, destfile="./data/cameras.csv")
#list the file [and folder?] contents of a directory
list.files("./data")
#timestamp
dateDownloaded <- date()
dateDownloaded
fileUrl <- "https://data.baltimorecity.gov/api/views/dz54-2aru/rows.xlsx?accessType=DOWNLOAD"
download.file(fileUrl, destfile="./data/cameras.xlsx")
dateDownload <- date()
library(xlsx)
cameraData <- read.xlsx("./data/cameras.xlsx",sheetIndex = 1, header=TRUE)
?download.file()
download.file(fileUrl, destfile="./data/cameras.xlsx", mode="wb")
dateDownload <- date()
cameraData <- read.xlsx("./data/cameras.xlsx",sheetIndex = 1, header=TRUE)
head(cameraData)
cameraDataSubset <- read.xlsx("./data/cameras.xlsx",
sheetIndex=1,
colIndex=colIndex,
rowIndex=rowIndex)
cameraDataSubset <- read.xlsx("./data/cameras.xlsx", sheetIndex=1, colIndex=colIndex, rowIndex=rowIndex)
colIndex <- 2:3
rowIndex <- 1:4
cameraDataSubset <- read.xlsx("./data/cameras.xlsx",
sheetIndex=1,
colIndex=colIndex,
rowIndex=rowIndex)
cameraDataSubset
install.packages("XML")
library(XML)
fileUrl <-  "http://www.w3schools.com/xml/simple.xml"
doc <- xmlTreeParse(fileUrl, useInternal=TRUE)
rootNode <- xmlRoot(doc)
xmlName(rootNode)
names(rootNode)
rootNode[1]
rootNode[[1]]
rootNode[[1]][[1]]
xmlSapply(rootNode, xmlValue)
xmlSApply(rootNode, xmlValue)
xpathSapply(rootNode,"//name",xmlValue)
xpathSApply(rootNode,"//name",xmlValue)
xpathSApply(rootNode,"//price",xmlValue)
fileUrl <- "http://espn.go.com/nfl/team/_/name/bal/baltimore-ravens"
doc <- htmlTreeParse(fileUrl, useInternal=TRUE)
scores <- xpathSApply(doc,"//li[@class='score']", xmlValue)
scores
teams <- xpathSApply(doc,"//li[@class='team-name']", xmlValue)
teams
scores <- xpathSApply(doc, "//li[@class='score']", xmlValue)
scores
scores <- xpathSApply(doc,"//li[@class='score']",xmlValue)
scores
install.packages("jsonlite")
library(jsonlite)
library(jsonlite)
jsonData <- fromJSON("https://api.github.com/users/jtleek/repos")
names(jsonData)
names(jsonData$owner)
jsonData$owner$login
iris
myjson <- toJSON(iris, pretty=TRUE)
cat(myjson)
head(myjson)
cat(myjson)
names(iris)
iris2 <- fromJSON(myjson)
head(iris2)
install.packages(data.table)
install.packages("data.table")
library(data.table)
DF = data.frame(x=rnorm(9),
y=rep(c("a","b","c"), each=3),
z=rnorm(9))
head(DF,3)
DT = data.table(x=rnorm(9),
y=rep(c("a","b","c"), each=3),
z=rnorm(9))
head(DT,3)
tables()
DT[2,]
DT[DT$y="a",]
DT[DT$y=="a",]
DT[c(2,3)]
DT[,c(2,3)]
k = {print(10); 5}
k
DT[,list(mean(x),sum(z))]
DT[,table(y)]
DT[,w:=z^2]
DT
DT2 <- DT
DT[, y:=2]
DT
DT2
DT[, m:={tmp <- (x+z); log2(tmp+5)}]
DT
DT[,a:=x+>0]
DT[,a:=x>0]
dt
DT
DT[,b:=mean(x+w),by=a]
DT
set.seed(123)
DT <- data.table(x=sample(letters[1:3],1E5, TRUE))
DT[,.N,by=x]
DT <- data.table(x=rep(c("a","b","c"),each=100), y=rnorm(300))
setkey(DT,x)
DT['a']
DT1 <- data.table(x=c('a','a','b','dt1') ,y=1:4)
DT2 <- data.table(x=c('a','b','dt2') ,y=5:7)
setkey(DT1, x); setkey(DT2, x)
merge(DT1,DT2)
DT1 <- data.table(x=c('a','a','b','dt1') ,y=1:4)
DT2 <- data.table(x=c('a','b','dt2') ,z=5:7)
setkey(DT1, x); setkey(DT2, x)
merge(DT1,DT2)
big_df <- data.frame(x=rnorm(1E6),y=rnorm(1E6))
file <- tempfile()
write.table(big_df, file=file, row.names=FALSE, col.names=TRUE, sep="\t", quote=FALSE)
system.time(fread(file))
system.time(read.table(file, header=TRUE,sep="\t"))
system.time(fread(file))
url <- "https://docs.google.com/spreadsheets/d/1jgPvqCilrKbs3QhkHvGD2Cb2_qEvfSpXYa3ovScCtzI/edit#gid=0"
g <- readGoogleSheet(url)
cleanGoogleTable <- function(dat, table=1, skip=0, ncols=NA, nrows=-1, header=TRUE, dropFirstCol=NA){
if(!is.data.frame(dat)){
dat <- dat[[table]]
}
if(is.na(dropFirstCol)) {
firstCol <- na.omit(dat[[1]])
if(all(firstCol == ".") || all(firstCol== as.character(seq_along(firstCol)))) {
dat <- dat[, -1]
}
} else if(dropFirstCol) {
dat <- dat[, -1]
}
if(skip > 0){
dat <- dat[-seq_len(skip), ]
}
if(nrow(dat) == 1) return(dat)
if(nrow(dat) >= 2){
if(all(is.na(dat[2, ]))) dat <- dat[-2, ]
}
if(header && nrow(dat) > 1){
header <- as.character(dat[1, ])
names(dat) <- header
dat <- dat[-1, ]
}
# Keep only desired columns
if(!is.na(ncols)){
ncols <- min(ncols, ncol(dat))
dat <- dat[, seq_len(ncols)]
}
# Keep only desired rows
if(nrows > 0){
nrows <- min(nrows, nrow(dat))
dat <- dat[seq_len(nrows), ]
}
# Rename rows
rownames(dat) <- seq_len(nrow(dat))
dat
}v
cleanGoogleTable <- function(dat, table=1, skip=0, ncols=NA, nrows=-1, header=TRUE, dropFirstCol=NA){
if(!is.data.frame(dat)){
dat <- dat[[table]]
}
if(is.na(dropFirstCol)) {
firstCol <- na.omit(dat[[1]])
if(all(firstCol == ".") || all(firstCol== as.character(seq_along(firstCol)))) {
dat <- dat[, -1]
}
} else if(dropFirstCol) {
dat <- dat[, -1]
}
if(skip > 0){
dat <- dat[-seq_len(skip), ]
}
if(nrow(dat) == 1) return(dat)
if(nrow(dat) >= 2){
if(all(is.na(dat[2, ]))) dat <- dat[-2, ]
}
if(header && nrow(dat) > 1){
header <- as.character(dat[1, ])
names(dat) <- header
dat <- dat[-1, ]
}
# Keep only desired columns
if(!is.na(ncols)){
ncols <- min(ncols, ncol(dat))
dat <- dat[, seq_len(ncols)]
}
# Keep only desired rows
if(nrows > 0){
nrows <- min(nrows, nrow(dat))
dat <- dat[seq_len(nrows), ]
}
# Rename rows
rownames(dat) <- seq_len(nrow(dat))
dat
}
readGoogleSheet <- function(url, na.string="", header=TRUE){
stopifnot(require(XML))
# Suppress warnings because Google docs seems to have incomplete final line
suppressWarnings({
doc <- paste(readLines(url), collapse=" ")
})
if(nchar(doc) == 0) stop("No content found")
htmlTable <- gsub("^.*?(<table.*</table).*$", "\\1>", doc)
ret <- readHTMLTable(htmlTable, header=header, stringsAsFactors=FALSE, as.data.frame=TRUE)
lapply(ret, function(x){ x[ x == na.string] <- NA; x})
}
url <- "https://docs.google.com/spreadsheets/d/1jgPvqCilrKbs3QhkHvGD2Cb2_qEvfSpXYa3ovScCtzI/edit#gid=0"
g <- readGoogleSheet(url)
cleanGoogleTable(g, table=1)
g
url <- "https://docs.google.com/spreadsheets/d/1jgPvqCilrKbs3QhkHvGD2Cb2_qEvfSpXYa3ovScCtzI/pubhtml"
g <- readGoogleSheet(url)
cleanGoogleTable(g, table=1)
g
gDataFrame <- cleanGoogleTable(g, table=1)
gDataFrame
setwd("~/Coursera/Coursera_GetCleanData/Week1")
print("ABC")
print(dateDownloaded)
fileUrl <- "https://d396qusza40orc.cloudfront.net/getdata%2Fdata%2Fss06hid.csv"
download.file(fileUrl, destfile="./data/community.csv")
dateDownloaded <- date()
print(dateDownloaded)
communityData <- read.table("./data/community.csv", sep=",", header=TRUE)
head(communityData)
names(communityData)
communityData$VAL
propertyValue <- communityData$VAL
milOrMore <- propertyValue[!is.na(propertyValue)]
milOrMore <- propertyValue[!is.na(propertyValue) & propertyValue == 24]
count(milOrMore)
length(milOrMore)
names(propertyValue)
names(communityData)
communityData$VES
fileUrl <- "https://d396qusza40orc.cloudfront.net/getdata%2Fdata%2FDATA.gov_NGAP.xlsx"
download.file(fileUrl, destfile="./data/gas.xlsx", mode="wb")
dateDownload <- date()
print(dateDownloaded)
library(xlsx)
gasData <- read.xlsx("./data/gas.xlsx",sheetIndex = 1, header=TRUE)
head(gasData)
colIndex <- 7:15
rowIndex <- 18:23
dat <- read.xlsx("./data/gas.xlsx",
sheetIndex=1,
colIndex=colIndex,
rowIndex=rowIndex)
sum(dat$Zip*dat$Ext,na.rm=T)
library(XML)
fileUrl <-  "https://d396qusza40orc.cloudfront.net/getdata%2Fdata%2Frestaurants.xml"
doc <- xmlTreeParse(fileUrl, useInternal=TRUE)
fileUrl <- "https://d396qusza40orc.cloudfront.net/getdata%2Fdata%2Frestaurants.xml"
doc <- xmlTreeParse(fileUrl, useInternal=TRUE)
fileUrl <- "http://d396qusza40orc.cloudfront.net/getdata%2Fdata%2Frestaurants.xml"
doc <- xmlTreeParse(fileUrl, useInternal=TRUE)
doc
rootNode <- xmlRoot(doc)
xmlName(rootNode)
names(rootNode)
xmlSapply(rootNode, xmlValue)
xpathSapply(rootNode,"//zipcode",xmlValue)
library(XML)
xmlSapply(rootNode, xmlValue)
?xmlSapply
xmlSApply(rootNode, xmlValue)
xpathSApply(rootNode,"//zipcode",xmlValue)
zipcodeData <- xpathSApply(rootNode,"//zipcode",xmlValue)
specificZipcode <- zipcodeData[zipcodeData == "21231"]
length(specificZipcode)
library(data.table)
fileUrl <- "https://d396qusza40orc.cloudfront.net/getdata%2Fdata%2Fss06pid.csv"
download.file(fileUrl, destfile="./data/community2.csv")
file <- "./data/community.csv"
DT <- fread(file)
system.time(DT[,mean(pwgtp15),by=SEX])
system.time(DT[,mean(pwgtp15),by=DT$SEX])
DT[,mean(pwgtp15),by=SEX]
mean(DT[DT$SEX==1,]$pwgtp15); mean(DT[DT$SEX==2,]$pwgtp15)
library(data.table)
DT[,mean(pwgtp15),by=SEX]
rowMeans(DT)[DT$SEX==1]; rowMeans(DT)[DT$SEX==2]
value = {rowMeans(DT)[DT$SEX==1]; rowMeans(DT)[DT$SEX==2]}
DT
rowMeans(DT)[DT$SEX==1]
DT$SEX
DT[SEX]
DT['SEX']
names(DT)
DT[,mean(pwgtp15), by=SEX]
DT[,Mean:=mean(pwgtp15), by=SEX]
fileUrl <- "https://d396qusza40orc.cloudfront.net/getdata%2Fdata%2Fss06pid.csv"
download.file(fileUrl, destfile="./data/community2.csv")
file <- "./data/community2.csv"
DT <- fread(file)
DT[,mean(pwgtp15), by=SEX]
system.time(DT[,mean(pwgtp15),by=SEX])
system.time(rowMeans(DT)[DT$SEX==1]; rowMeans(DT)[DT$SEX==2])
system.time({rowMeans(DT)[DT$SEX==1]; rowMeans(DT)[DT$SEX==2]})
rowMeans(DT)[DT$SEX==1]
mean(DT[DT$SEX==1,]$pwgtp15)
system.time({mean(DT[DT$SEX==1,]$pwgtp15); mean(DT[DT$SEX==2,]$pwgtp15)})
mean(DT$pwgtp15,by=DT$SEX)
system.time(mean(DT$pwgtp15,by=DT$SEX))
sapply(split(DT$pwgtp15,DT$SEX),mean)
system.time(sapply(split(DT$pwgtp15,DT$SEX),mean))
tapply(DT$pwgtp15,DT$SEX,mean)
system.time(tapply(DT$pwgtp15,DT$SEX,mean))
tapply(DT$pwgtp15,DT$SEX,mean)
sapply(split(DT$pwgtp15,DT$SEX),mean)
